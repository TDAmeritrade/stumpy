{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Matrix Profiles for Streaming Time Series Data\n",
    "\n",
    "Now that you have a basic understanding of how to compute a matrix profile, in this short tutorial, we will demonstrate how to incrementally update your matrix profile when you have streaming (on-line) data using the `stumpy.stumpi` (\"STUMP Incremental\") function. You can learn more about the details of this approach by reading Section G of the [Matrix Profile I](https://www.cs.ucr.edu/~eamonn/PID4481997_extend_Matrix%20Profile_I.pdf) paper and Section 4.6 and Table 5 [this paper](https://www.cs.ucr.edu/~eamonn/ten_quadrillion.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Let's import the packages that we'll need to create and analyze a randomly generated time series data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import stumpy\n",
    "import numpy.testing as npt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Some Random Time Series Data\n",
    "\n",
    "Imagine that we have an [IoT](https://en.wikipedia.org/wiki/Internet_of_things) sensor that has been collecting data once an hour for the last 14 days. That would mean that we've amassed `14 * 24 = 336` data points up until this point and our data set might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.random.rand(336)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, perhaps, we know from experience that an interesting motif or anomaly might be detectable within a 12 hour (sliding) time window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Batch Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the matrix profile using a batch process is straightforward using `stumpy.stump`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = stumpy.stump(T, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as the length of `T` grows with each passing hour, it will take increasingly more time to compute the matrix profile since `stumpy.stump` will actually re-compute all of the pairwise distances between all subsequences within the time series. This is super time consuming! Instead, for streaming data, we want to find a way to take the new incoming (single) data point and compare the subsequence that it resides in with the rest of the time series (i.e., compute the distance profile) and update the existing matrix profile. Luckily, this can be easily accomplished with `stumpy.stumpi` or \"STUMP Incremental\".\n",
    "\n",
    "## Streaming (On-line) Analysis with STUMPI\n",
    "\n",
    "As we wait for the next data point, `t`, to arrive, we can take our existing data set and initialize a set of inputs that are necessary for incrementally updating our matrix profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, P, I, QT, M_T, Σ_T = stumpy.stumpi_init(T, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here: \n",
    "\n",
    "* `T` is our original time series but with all `nan`/`inf` values set to zero (`nan`/`inf` are not supported)\n",
    "* `P` is the matrix profile\n",
    "* `I` is the matrix profile index\n",
    "* `QT` is the sliding window dot product for the last subsequence in `T`\n",
    "* `M_T` is the sliding window mean for all subsequences in `T` \n",
    "* `Σ_T` is the sliding window standard deviation for all subsequences in `T`\n",
    "\n",
    "And when a new data point, `t`, arrives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.random.rand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the `stumpy.stumpi` function to quickly and easily update the matrix profile, `P`, and matrix profile indices, `I`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, P, I, QT, M_T, Σ_T = stumpy.stumpi(t, T, m, P, I, QT, M_T, Σ_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, with each `t`, all of the values that are returned by `stumpy.stumpi` (i.e., `T, P, I, QT, M_T, Σ_T`) will also increase in length by a single element in order to accomodate the newly added subsequence. For example, if we look at the length of, say, `T`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice that the length of `T` has increased from `336` to `337` since `t` has been appended to the time series and all of the other inputs have been updated accordingly as well. So, not only does `stumpy.stumpi` compare the new subsequence with all of the existing ones and updates the historical values but it also determines which one of the existing subsequences is the nearest neighbor to the new subsequence and appends this information to the matrix profile. And this can continue on, say, for another 1,000 iterations (or indefinitely) as additional data is streamed in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    t = np.random.rand()\n",
    "    T, P, I, QT, M_T, Σ_T = stumpy.stumpi(t, T, m, P, I, QT, M_T, Σ_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to reiterate that incremental `stumpy.stumpi` is different from batch `stumpy.stump` in that it does <b><u>not</u></b> waste any time re-computing any of the past pairwise distances. `stumpy.stumpi` only spends time computing <b><u>new</u></b> distances and then updates the appropriate arrays where necessary and, thus, it is really fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "### The Matrix Profile\n",
    "\n",
    "Now, this claim of \"fast updating\" with streaming (on-line) data may feel strange or seem magical so, first, let's validate that the output from incremental `stumpy.stumpi` is the same as performing batch `stumpy.stump`. Let's start with the full time series with `64` data points and compute the full matrix profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_full = np.random.rand(64)\n",
    "m = 8\n",
    "\n",
    "mp = stumpy.stump(T_full, m)\n",
    "P_full = mp[:, 0]\n",
    "I_full = mp[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for `stumpy.stumpi`, we'll only start with the first `10` elements from the full length time series and then incrementally stream in the additional data points one at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with half of the full length time series and initialize inputs\n",
    "T_stream = T_full[:10].copy()\n",
    "T_stream, P_stream, I_stream, QT_stream, M_T_stream, Σ_T_stream = stumpy.stumpi_init(T_stream, m)\n",
    "\n",
    "# Incrementally add one new data point at a time until `T_stream == T_full` and update the matrix profile\n",
    "for i in range(len(T_stream), len(T_full)):\n",
    "    t = T_full[i]\n",
    "    T_stream, P_stream, I_stream, QT_stream, M_T_stream, Σ_T_stream = stumpy.stumpi(t, T_stream, m, P_stream, I_stream, QT_stream, M_T_stream, Σ_T_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're done, let's check and validate that:\n",
    "\n",
    "1. `T_stream == T_full`\n",
    "2. `P_stream == P_full`\n",
    "3. `I_stream == I_full`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "npt.assert_almost_equal(T_stream, T_full)\n",
    "npt.assert_almost_equal(P_stream, P_full)\n",
    "npt.assert_almost_equal(I_stream, I_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no errors! So, this means that `stump.stumpi` indeed produces the correct matrix profile results that we'd expect.\n",
    "\n",
    "### The Performance\n",
    "\n",
    "We've basically claimed that incrementally updating our matrix profile with `stumpy.stumpi` is much faster (in total computation time) than performing a full pairwise distance calculation with `stumpy.stump` as each new data point arrives. Let's actually compare the timings by taking a full time series that is 1,000 data points in length and we initialize both approaches with the first 20% of the time series (i.e., the first 200 points) and append a single new data point at each iteration before re-computing the matrix profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stumpy.stump: 460.8s\n",
      "stumpy.stumpi:, 2.6s\n"
     ]
    }
   ],
   "source": [
    "T_full = np.random.rand(1000)\n",
    "T_stream = T_full[:200].copy()\n",
    "m = 10\n",
    "\n",
    "# `stumpy.stump` timing\n",
    "start = time.time()\n",
    "mp = stumpy.stump(T_stream, m)\n",
    "for i in range(200, len(T_full)):\n",
    "    T_stream = np.append(T_stream, T_full[i])\n",
    "    mp = stumpy.stump(T_stream, m)\n",
    "stump_time = time.time() - start\n",
    "\n",
    "# `stumpy.stumpi` timing\n",
    "start = time.time()\n",
    "T_stream, P, I, QT, M_T, Σ_T = stumpy.stumpi_init(T_stream, m)\n",
    "for i in range(200, len(T_full)):\n",
    "    t = T_full[i]\n",
    "    T_stream, P, I, QT, M_T, Σ_T = stumpy.stumpi(t, T_stream, m, P, I, QT, M_T, Σ_T)\n",
    "stumpi_time = time.time() - start\n",
    "\n",
    "print(f\"stumpy.stump: {np.round(stump_time,1)}s\")\n",
    "print(f\"stumpy.stumpi:, {np.round(stumpi_time, 1)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting aside the fact that having more CPUs will speed up both approaches, we clearly see that incremental `stumpy.stumpi` (`2.6s` of total computational time) is several orders of magnitude faster than batch `stumpy.stump` (`460s` of total computational time) for processing streaming data. In fact for the current hardware, on average, it is taking roughly `460.8s / 800 = 0.576s` for `stumpy.stump` to analyze each new matrix profile. So, if you have a new data point arriving every once every half a second, then you wouldn't be able to keep up. However, with an average analysis time of `2.6s / 800 = 0.00325s`, `stumpy.stumpi` should be able to comfortably handle and process ~300 new data points per second using fairly modest hardware. Additionally, batch `stumpy.stump` will get even slower as more and more data points get appended to the existing time series while `stumpy.stumpi` will continue to be highly performant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats\n",
    "\n",
    "As with all software, there are a few things to keep in mind when using `stumpy.stumpi`: \n",
    "\n",
    "1. There is currently no support for time series with that contain `NaN`/`inf` values\n",
    "2. This only works for self-joins and not AB-joins\n",
    "3. Removing data points from the front/head of the time series should be avoided since the matrix profile indices are recorded based on the absolute subsequence position within the time series "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "And that's it! You've just learned how to incrementally update your matrix profile for streaming (on-line) data. \n",
    "\n",
    "## Resources\n",
    "​\n",
    "[Matrix Profile I](https://www.cs.ucr.edu/~eamonn/PID4481997_extend_Matrix%20Profile_I.pdf)\n",
    "\n",
    "[Time Series Joins, Motifs, Discords and Shapelets:\n",
    "A Unifying View that Exploits the Matrix Profile](https://www.cs.ucr.edu/~eamonn/MP_journal.pdf) (see Section 4.6 and Table 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
